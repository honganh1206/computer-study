
[[Definition and examples of LLMs]]

[[Tokenization]]

[[Evaluating LLMs]]



## Summarized


[02:21](https://www.youtube.com/watch?v=9vM4p9NN0Ts&t=141s) [[Focus on data evaluation and systems in industry over architecture]] 

[06:25](https://www.youtube.com/watch?v=9vM4p9NN0Ts&t=385s) [[Auto regressive language models predict the next word in a sentence]]. 

[08:26](https://www.youtube.com/watch?v=9vM4p9NN0Ts&t=506s) Tokenizing text is crucial for language models 

[12:38](https://www.youtube.com/watch?v=9vM4p9NN0Ts&t=758s) Training a large language model involves using a large corpus of text. 

[14:49](https://www.youtube.com/watch?v=9vM4p9NN0Ts&t=889s) Tokenization process considerations 

[18:40](https://www.youtube.com/watch?v=9vM4p9NN0Ts&t=1120s) Tokenization improvement in GPT 4 for code understanding 

[20:31](https://www.youtube.com/watch?v=9vM4p9NN0Ts&t=1231s) Perplexity measures model hesitation between tokens 

[24:18](https://www.youtube.com/watch?v=9vM4p9NN0Ts&t=1458s) Comparing outputs and model prompting 

[26:15](https://www.youtube.com/watch?v=9vM4p9NN0Ts&t=1575s) Evaluation of language models can yield different results 

[30:15](https://www.youtube.com/watch?v=9vM4p9NN0Ts&t=1815s) Challenges in training large language models 

[32:06](https://www.youtube.com/watch?v=9vM4p9NN0Ts&t=1926s) Challenges in building large language models

[35:57](https://www.youtube.com/watch?v=9vM4p9NN0Ts&t=2157s) Collecting real-world data is crucial for large language models

[37:53](https://www.youtube.com/watch?v=9vM4p9NN0Ts&t=2273s&pp=0gcJCTAAlc8ueATH) Challenges in building large language models [41:38](https://www.youtube.com/watch?v=9vM4p9NN0Ts&t=2498s) Scaling laws predict performance improvement with more data and larger models [43:33](https://www.youtube.com/watch?v=9vM4p9NN0Ts&t=2613s) Relationship between data, parameters, and compute [47:21](https://www.youtube.com/watch?v=9vM4p9NN0Ts&t=2841s) Importance of scaling laws in model performance [49:12](https://www.youtube.com/watch?v=9vM4p9NN0Ts&t=2952s) Quality of data matters more than architecture and losses in scaling laws [52:54](https://www.youtube.com/watch?v=9vM4p9NN0Ts&t=3174s) Inference for large language models is very expensive [54:54](https://www.youtube.com/watch?v=9vM4p9NN0Ts&t=3294s) Training large language models is costly [59:12](https://www.youtube.com/watch?v=9vM4p9NN0Ts&t=3552s) Post training aligns language models for AI assistant use [1:01:05](https://www.youtube.com/watch?v=9vM4p9NN0Ts&t=3665s) Supervised fine-tuning for large language models [1:04:50](https://www.youtube.com/watch?v=9vM4p9NN0Ts&t=3890s) Leveraging large language models for data generation and synthesis [1:06:49](https://www.youtube.com/watch?v=9vM4p9NN0Ts&t=4009s) Balancing data generation and human input for effective learning [1:10:23](https://www.youtube.com/watch?v=9vM4p9NN0Ts&t=4223s) Limitations of human abilities in generating large language models [1:12:12](https://www.youtube.com/watch?v=9vM4p9NN0Ts&t=4332s) Training language models to maximize human preference instead of cloning human behaviors. [1:16:06](https://www.youtube.com/watch?v=9vM4p9NN0Ts&t=4566s) Training reward model using softmax logits for human preferences. [1:18:02](https://www.youtube.com/watch?v=9vM4p9NN0Ts&t=4682s) Modeling optimization and challenges in large language models (LLMs) [1:21:49](https://www.youtube.com/watch?v=9vM4p9NN0Ts&t=4909s) Reinforcement learning models and potential benefits [1:23:44](https://www.youtube.com/watch?v=9vM4p9NN0Ts&t=5024s) Challenges with using humans for data annotation [1:27:21](https://www.youtube.com/watch?v=9vM4p9NN0Ts&t=5241s) LLMs are cost-effective and have better agreement with humans than humans themselves [1:29:12](https://www.youtube.com/watch?v=9vM4p9NN0Ts&t=5352s) Perplexity is not calibrated for large language models [1:33:00](https://www.youtube.com/watch?v=9vM4p9NN0Ts&t=5580s) Variance in performance of GPT-4 based on prompt specificity [1:34:51](https://www.youtube.com/watch?v=9vM4p9NN0Ts&t=5691s&pp=0gcJCTAAlc8ueATH) Pre-training data plays a vital role in model initialization [1:38:32](https://www.youtube.com/watch?v=9vM4p9NN0Ts&t=5912s&pp=0gcJCTAAlc8ueATH) Utilize GPUs efficiently with matrix multiplication [1:40:21](https://www.youtube.com/watch?v=9vM4p9NN0Ts&t=6021s) Utilizing 16 bits for faster training in deep learning [1:44:08](https://www.youtube.com/watch?v=9vM4p9NN0Ts&t=6248s) Building Large Language Models from scratch